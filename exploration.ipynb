{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyArrow\n",
    "\n",
    "- https://arrow.apache.org/\n",
    "- Pasos de instalación\n",
    "- Descripción del producto, modo de funcionamiento, mostrar cómo describir los esquemas\n",
    "- Mostrar cómo importar los datos de Stackoverflow\n",
    "- Mostrar posibles optimizaciones para realizar RQ1 a RQ4 vistos en la sesión 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos de Instalación\n",
    "\n",
    "La instalación de PyArrow (a 26 de noviembre de 2024 se recomienda la versión `18.1`) es simple, y viene detallada en su [página web](https://arrow.apache.org/install/), estando disponible (a la misma fecha) para Python desde su versión `3.9` hasta la `3.13`.\n",
    "\n",
    "Existen tres paquetes de PyArrow disponibles para Python, con menor o mayor cantidad de componentes de Arrow integrados, que podemos instalar, aunque también podemos instalar estos componentes individualmente. Estos paquetes son los siguientes:\n",
    "\n",
    "**Componente** | **Paquete** | `pyarrow-core` | `pyarrow` | `pyarrow-all`\n",
    "---------|----------|:--------:|:-------:|:-------:\n",
    "Core | pyarrow-core | ✅ | ✅ | ✅\n",
    "Parquet | libparquet |  | ✅ | ✅\n",
    "Dataset | libarrow-dataset |  | ✅ | ✅\n",
    "Acero | libarrow-acero |  | ✅ | ✅\n",
    "Substrait | libarrow-substrait |  | ✅ | ✅\n",
    "Flight | libarrow-flight |  |  | ✅\n",
    "Flight SQL | libarrow-flight-SQL |  |  | ✅\n",
    "Gandiva | libarrow-gandiva |  |  | ✅\n",
    "\n",
    "Según el entorno de desarrollo y el entorno virtual que utilicemos usaremos **pip** o **conda** para instalar el paquete:\n",
    "\n",
    "- Instalación con **pip**: `pip install pyarrow==18.1.*`\n",
    "- Instalación con **conda**: `conda install pyarrow=18.1.* -c conda-forge`\n",
    "\n",
    "En la instalación se incluyen los binarios de las librerías `Apache Arrow` y `Apache Parquet` de C++. \n",
    "\n",
    "PyArrow tiene las siguientes dependencias opcionales:\n",
    "- `NumPyy 1.16.6` o mayor\n",
    "- `pandas 1.0` o mayor\n",
    "- `cffi` (permite interactuar con código de C en Python)\n",
    "\n",
    "Además también podemos acceder al código fuente de Arrow y sus diferentes implementaciones en su [repositorio de Github](https://github.com/apache/arrow).\n",
    "\n",
    "### Instalación de Librerías Necesarias \n",
    "A continuación, instalamos las librerías necesarias para ejecutar el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.12/site-packages (18.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es PyArrow?\n",
    "PyArrow es la implementación de **Apache Arrow** para **Python**, por lo que para poder explicar qué es PyArrow debemos conocer primero Apache Arrow.\n",
    "\n",
    "Apache Arrow es, según su página web, una plataforma de desarrollo de software para construir aplicaciones de gran rendimiento que procesan y transportan grandes conjuntos de datos. Está diseñado para mejorar el rendimiento de los algoritmos de análisis y para eficiencia al mover datos de un sistema o lenguaje de programación a otro. \n",
    "\n",
    "En la práctica, se utiliza para mejorar el rendimiento y la interoperabilidad entre diferentes herramientas de procesamiento de datos dado que el formato Arrow tiene implementación en la mayoría de lenguajes de programación modernos y más utilizados, como Python, R, Julia, Rust, Go, Java, C++, C, C# o Ruby.\n",
    "\n",
    "Arrow destaca por la forma en la que almacena los datos en memoría, ya que lo hace en formato columnar, lo que permite un acceso más rápido y eficiente en grandes cargas de trabajo analíticas, permitiendo representar conjuntos de datos estructurados en formatos semejantes a tablas. Además, es *language-agnostic*, por lo que soporta diferentes lenguajes y facilita el intercambio de datos entre diferentes aplicaciones sin la necesidad de una gran cantidad de recursos. También aseguran la retrocompatibilidad del formato al añadir solo actualizaciones no destructivas, como la adición de nuevos tipos de datos.\n",
    "\n",
    "PyArrow es la implementación de Apache Arrow en Python. Es una biblioteca que permite trabajar con el formato Arrow y su ecosistema desde Python, y está construida sobre la librería de Arrow en C++ (similar a lo que ocurre con la librería `numpy`), permitiendo así un tratamiento de datos más rápido.\n",
    "\n",
    "PyArrow permite la creación y manipulación de estructuras de datos Arrow directamente en Python, permitiendo trabajar con ellas con operaciones paralelas y destacando su fácil conversión en `DataFrames` de la librería `pandas` y con `pySpark`, por lo que permite enlazar con las técnicas de análisis de datos distribuidos que ofrece Spark. De igual forma, PyArrow permite trabajar con los formatos de datos más comunes, como son CSV y Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de Esquemas\n",
    "\n",
    "Un `Schema`, o esquema, en PyArrow es una estructura que describe la organización de los datos en una tabla o un set de datos. Es esencialmente una representación de los metadatos de las columnas, definiendo aspectos como sus nombres, tipo de datos y, opcionalmente, metadatos adicionales.\n",
    "\n",
    "Los esquemas son útiles para garantizar la coherencia en el manejo de datos y para validar que los datos cumplan con una estructura predefinida antes de ser procesados. En este aspecto, son muy similares a los esquemas definidos en SQL.\n",
    "\n",
    "Definimos un esquema usando función `schema` de PyArrow, que genera un objeto de la clase `Schema`, donde introducimos los nombres de las columnas, los tipos de éstas, y opcionalmente metadatos en forma de pares clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nombre: string\n",
       "edad: int32\n",
       "peso: float\n",
       "-- schema metadata --\n",
       "descripcion: 'Esquema que representa los datos de una persona'\n",
       "peso: 'En kilogramos'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "sc = pa.schema([\n",
    "    (\"nombre\", pa.string()),\n",
    "    (\"edad\", pa.int32()),\n",
    "    (\"peso\", pa.float32()),\n",
    "], metadata = {\n",
    "    \"descripcion\": \"Esquema que representa los datos de una persona\",\n",
    "    \"peso\" : \"En kilogramos\"\n",
    "})\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos asociar esquemas a `Table`s, las estructuras principales de PyArrow que veremos a continuación, de forma que se verifiquen los datos que se importen. En caso de que los datos no coincidan con el esquema definido, PyArrow lanzará un error. Es decir, usando esquemas aseguramos la coherencia de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nombre: string\n",
       "edad: int32\n",
       "peso: float\n",
       "-- schema metadata --\n",
       "descripcion: 'Esquema que representa los datos de una persona'\n",
       "peso: 'En kilogramos'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"nombre\": [\"Maria\", \"Luis\"],\n",
    "    \"edad\": [25, 30],\n",
    "    \"peso\": [65.8, 85.6]\n",
    "}\n",
    "\n",
    "table = pa.Table.from_pydict(data, schema=sc)\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden *castear* una tabla para forzarla a adoptar un nuevo esquema, variando los tipos de datos de las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "nombre: string\n",
       "edad: int8\n",
       "peso: float\n",
       "----\n",
       "nombre: [[\"Maria\",\"Luis\"]]\n",
       "edad: [[25,30]]\n",
       "peso: [[65.8,85.6]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_int8 = pa.schema([\n",
    "    (\"nombre\", pa.string()),\n",
    "    (\"edad\", pa.int8()),\n",
    "    (\"peso\", pa.float32()),\n",
    "])\n",
    "\n",
    "table_int8 = table.cast(sc_int8)\n",
    "table_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los esquemas son inmutables, pero podemos crear esquemas nuevos al aplicar transformaciones a esquemas ya definidos para definir nuevas columnas, eliminarlas, cambiar los metadatos, cambiar el tipo de datos de una columna e incluso unir esquemas (mantiene los metadatos del primer esquema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nombre: string\n",
       "edad: int32\n",
       "altura: float\n",
       "-- schema metadata --\n",
       "descripcion: 'Esquema que representa los datos de una persona'\n",
       "altura: 'en metros'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc2 = (\n",
    "    sc.remove(2)\n",
    "    .append(pa.field(\"altura\", pa.float32()))\n",
    "    .with_metadata(\n",
    "        {\n",
    "            \"descripcion\": \"Esquema que representa los datos de una persona\",\n",
    "            \"altura\": \"en metros\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "sc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nombre: string\n",
       "edad: int32\n",
       "peso: float\n",
       "pais: string\n",
       "municipio: string\n",
       "zipcode: int16\n",
       "puerta: string\n",
       "-- schema metadata --\n",
       "descripcion: 'Esquema que representa los datos de una persona'\n",
       "peso: 'En kilogramos'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc3 = pa.schema([\n",
    "    (\"pais\", pa.string()),\n",
    "    (\"municipio\", pa.string()),\n",
    "    (\"zipcode\", pa.int16()),\n",
    "    (\"puerta\", pa.string()),\n",
    "    ], metadata = {\n",
    "        \"cosas\": \"cosas\",\n",
    "    }\n",
    ")\n",
    "# Vease que sc no ha cambiado\n",
    "uni_sc = pa.unify_schemas([sc,sc3])\n",
    "uni_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionamiento\n",
    "Como hemos dicho, PyArrow es un paquete de Python que funciona como una interfaz para manejar datos usando el formato Apache Arrow. Al funcionar sobre la librería de Apache Arrow para C++, es rápido y eficiente a la hora de manejar datos tanto en la memoría como en el disco, al aprovechar las ventajas tanto de Apache Arrow como de C++.\n",
    "\n",
    "#### Estructuras de Datos Básicas\n",
    "Las dos estructuras básicas para manejar datos de PyArrow son las `Table`s y los `Array`s. Una `Table` representa datos en forma de tabla, de forma similar a los `DataFrames` de pandas, mientras `Array` representaría una única columna de datos con un único tipo, de forma paralela a las `Series` de pandas. Sin embargo, existe una diferencia principal entre ellas, y es que las estructuras de PyArrow son **inmutables**, lo cual es clave a la hora de garantizar la integridad de los datos y mejorar el rendimiento.\n",
    "\n",
    "Como hemos visto, podemos crear una `Table` a partir de  un diccionario de Python, de forma muy parecida a como lo hacíamos en pandas, y un `Array` a partir de una lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "manzanas: int64\n",
       "kiwis: int64\n",
       "limones: int64\n",
       "brevas: int64\n",
       "----\n",
       "manzanas: [[2,7,3,4]]\n",
       "kiwis: [[3,2,1,5]]\n",
       "limones: [[2,3,8,9]]\n",
       "brevas: [[6,3,1,9]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "array = pa.array([1, 2, 3, 4, 5])\n",
    "\n",
    "ex_dict = {\n",
    "    \"manzanas\": [2,7,3,4],\n",
    "    \"kiwis\": [3,2,1,5],\n",
    "    \"limones\": [2,3,8,9],\n",
    "    \"brevas\": [6,3,1,9],\n",
    "}\n",
    "\n",
    "table = pa.Table.from_pydict(ex_dict)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos generar `Table`s mediante la combinación de diferentes `Array`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "manzanas: int64\n",
       "kiwis: int64\n",
       "limones: int64\n",
       "brevas: int64\n",
       "----\n",
       "manzanas: [[10,9,11,12,13,8,12,9,4,14]]\n",
       "kiwis: [[4,10,11,5,8,14,11,9,5,12]]\n",
       "limones: [[9,12,13,10,6,16,6,9,11,9]]\n",
       "brevas: [[12,8,15,15,14,10,9,9,9,6]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42069)\n",
    "arrays = [list(map(round, np.random.normal(10,3,10))) for _ in range(4)]\n",
    "table = pa.Table.from_arrays(arrays, names = ['manzanas', 'kiwis', 'limones', 'brevas'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, la presentación de las tablas de PyArrow no es tan clara y estelística como podríamos desear. Podemos solucionar esto mediante una de las grandes ventajas de PyArrow, que es su compatibilidad con pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manzanas</th>\n",
       "      <th>kiwis</th>\n",
       "      <th>limones</th>\n",
       "      <th>brevas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manzanas  kiwis  limones  brevas\n",
       "0        10      4        9      12\n",
       "1         9     10       12       8\n",
       "2        11     11       13      15\n",
       "3        12      5       10      15\n",
       "4        13      8        6      14\n",
       "5         8     14       16      10\n",
       "6        12     11        6       9\n",
       "7         9      9        9       9\n",
       "8         4      5       11       9\n",
       "9        14     12        9       6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = table.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta compatibilidad es bidireccional, es decir, también podemos una `Table` de PyArrow desde un `DataFrame` de pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "manzanas: int64\n",
       "kiwis: int64\n",
       "limones: int64\n",
       "brevas: int64\n",
       "----\n",
       "manzanas: [[10,9,11,12,13,8,12,9,4,14]]\n",
       "kiwis: [[4,10,11,5,8,14,11,9,5,12]]\n",
       "limones: [[9,12,13,10,6,16,6,9,11,9]]\n",
       "brevas: [[12,8,15,15,14,10,9,9,9,6]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = pa.Table.from_pandas(df)\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, debemos mencionar los `Dataset`s de PyArrow, los cuales son abstracciones diseñadas para trabajar con colecciones de tablas distribuidas en múltiples archivos o particiones, permitiendo la compatibilidad con sistemas distribuidos como HDFS. Veremos más sobre ellos más tarde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura y Lectura de Datos\n",
    "PyArrow es compatible con una gran cantidad de formatos, y la lectura de datos se realiza de forma muy similar en todas ellas. Vamos a destacar y detallar algunas de ellas especialmente interesantes.\n",
    "\n",
    "#### **CSV**\n",
    "Es uno de los formatos más comúnmente usados para guardar datos, por lo que su estudio es relevante. Además, las opciones que vemos con los archivos CSV se pueden generar a otros formatos. \n",
    "\n",
    "**Escritura**\n",
    "\n",
    "Para poder usar el módulo `csv` de PyArrow primero debemos importarlo, aunque no lo usemos explícitamente, o bien podemos importar las funciones a utilizar directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.csv import WriteOptions, write_csv\n",
    "\n",
    "write_csv(\n",
    "    data=table,\n",
    "    output_file=\"frutas.csv\",\n",
    "    write_options=WriteOptions(include_header=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lectura**\n",
    "\n",
    "Para leer los datos podemos especificar un esquema mediante la clase `ConvertOptions`, o en caso de no hacerlo, PyArrow tratará de inferir los tipos de las diferentes columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "manzanas: int64\n",
       "kiwis: int64\n",
       "limones: int64\n",
       "brevas: int64\n",
       "----\n",
       "manzanas: [[10,9,11,12,13,8,12,9,4,14]]\n",
       "kiwis: [[4,10,11,5,8,14,11,9,5,12]]\n",
       "limones: [[9,12,13,10,6,16,6,9,11,9]]\n",
       "brevas: [[12,8,15,15,14,10,9,9,9,6]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.csv import read_csv\n",
    "\n",
    "read_table = read_csv(\"frutas.csv\")\n",
    "read_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "manzanas: int8\n",
       "kiwis: int8\n",
       "limones: int8\n",
       "brevas: int8\n",
       "----\n",
       "manzanas: [[10,9,11,12,13,8,12,9,4,14]]\n",
       "kiwis: [[4,10,11,5,8,14,11,9,5,12]]\n",
       "limones: [[9,12,13,10,6,16,6,9,11,9]]\n",
       "brevas: [[12,8,15,15,14,10,9,9,9,6]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.csv import ConvertOptions\n",
    "\n",
    "schema = pa.schema( [\n",
    "        (\"manzanas\", pa.int8()),\n",
    "        (\"kiwis\", pa.int8()),\n",
    "        (\"limones\", pa.int8()),\n",
    "        (\"brevas\", pa.int8()),\n",
    "    ])\n",
    "\n",
    "fruits_table = read_csv(\"frutas.csv\", \n",
    "                      convert_options=ConvertOptions(column_types=schema))\n",
    "fruits_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feather**\n",
    "\n",
    "El formato Feather es un formato de almacenamiento de datos tabulares desarrollado conjuntamente por Apache Arrow y pandas.\n",
    "\n",
    "Está diseñado para ser un formato rápido, eficiente y ligero, siendo particularmente útil en flujos de trabajo donde se necesita transferir grandes volúmenes de datos entre Python y cualquier otro lenguaje que soporte Apache Arrow, manteniendo el rendimiento y la interoperabilidad.\n",
    "\n",
    "Su uso es extremadamente simple, tanto en la escritura como en la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "day: int64\n",
       "month: int64\n",
       "----\n",
       "day: [[11,8,20,21,14,22,5,27,28,22],[29,30,18,17,8,21,6,29,18,12],...,[16,6,13,26,25,13,3,25,10,22],[29,28,30,7,18,30,24,17,22,19]]\n",
       "month: [[11,1,8,8,2,11,6,3,11,7],[3,6,9,4,11,7,9,5,2,9],...,[3,1,1,5,9,1,6,8,3,11],[5,4,10,9,7,1,5,8,11,11]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.feather as ft\n",
    "\n",
    "# Escritura\n",
    "ft.write_feather(table, \"años.feather\")\n",
    "# Lectura\n",
    "feather_table = ft.read_table(\"años.feather\")\n",
    "feather_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Particiones**\n",
    "\n",
    "Cuando tenemos un set de datos grande podemos querer guardarlo en diferentes archivos. Esto se puede hacer automáticamente en PyArrow utilizando los `Dataset`s.\n",
    "\n",
    "Para ello debemos especificar un esquema que diga a PyArrow por qué columna queremos particionar nuestro dataset y el formato de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "base_dir = \"./particionado\"\n",
    "# Set de fechas con 10 entradas para cada año entre el 2000 y el 2009\n",
    "data = pa.table({\"day\": np.random.randint(1, 31, size=100),\n",
    "                 \"month\": np.random.randint(1, 12, size=100),\n",
    "                 \"year\": [2000 + x // 10 for x in range(100)]})\n",
    "\n",
    "ds.write_dataset(data, base_dir=base_dir, format=\"csv\",\n",
    "                 partitioning=ds.partitioning(pa.schema([(\"year\",pa.int16())])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos importar el dataset completo especificando el directorio base y luego convertirlo a una tabla con todos los datos que lo componen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos que componen el dataset:\n",
      "['./particionado/2000/part-0.csv', './particionado/2001/part-0.csv', './particionado/2002/part-0.csv', './particionado/2003/part-0.csv', './particionado/2004/part-0.csv', './particionado/2005/part-0.csv', './particionado/2006/part-0.csv', './particionado/2007/part-0.csv', './particionado/2008/part-0.csv', './particionado/2009/part-0.csv']\n",
      "\n",
      "\n",
      "Tabla leída:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "day: int64\n",
       "month: int64\n",
       "----\n",
       "day: [[11,8,20,21,14,22,5,27,28,22],[29,30,18,17,8,21,6,29,18,12],...,[16,6,13,26,25,13,3,25,10,22],[29,28,30,7,18,30,24,17,22,19]]\n",
       "month: [[11,1,8,8,2,11,6,3,11,7],[3,6,9,4,11,7,9,5,2,9],...,[3,1,1,5,9,1,6,8,3,11],[5,4,10,9,7,1,5,8,11,11]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.dataset(base_dir, format='csv')\n",
    "print(\"Archivos que componen el dataset:\")\n",
    "print(dataset.files)\n",
    "table = dataset.to_table()\n",
    "print(\"\\n\\nTabla leída:\")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir un dataset a una tabla directamente es peligroso, dado que generalmente en los casos que usemos datasets serán en los que la tabla completa sea extremadamente grande y posiblemente no quepa completa en memoria. Es por ello que es más seguro trabajar con el dataset por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [11,8,20,21,14,22,5,27,28,22]\n",
      "month: [11,1,8,8,2,11,6,3,11,7]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [29,30,18,17,8,21,6,29,18,12]\n",
      "month: [3,6,9,4,11,7,9,5,2,9]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [8,25,19,2,21,17,25,17,5,2]\n",
      "month: [8,11,8,5,5,9,11,8,4,9]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [24,22,30,5,15,5,27,2,3,14]\n",
      "month: [5,11,8,8,2,5,8,11,2,6]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [5,22,4,9,30,22,19,16,7,20]\n",
      "month: [11,9,9,7,6,4,3,2,3,6]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [24,16,13,13,11,12,27,3,16,1]\n",
      "month: [2,1,4,7,9,9,9,8,4,1]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [1,14,12,22,26,26,12,24,18,24]\n",
      "month: [1,7,6,9,7,2,4,4,3,1]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [29,4,16,2,20,24,20,24,27,24]\n",
      "month: [2,6,4,5,1,4,7,7,4,2]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [16,6,13,26,25,13,3,25,10,22]\n",
      "month: [3,1,1,5,9,1,6,8,3,11]\n",
      "pyarrow.RecordBatch\n",
      "day: int64\n",
      "month: int64\n",
      "----\n",
      "day: [29,28,30,7,18,30,24,17,22,19]\n",
      "month: [5,4,10,9,7,1,5,8,11,11]\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.to_batches():\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones con PyArrow\n",
    "\n",
    "Las operaciones se realizan en memoria directamente sobre las estructuras de datos de Arrow, utilizando funciones optimizadas para el procesamiento en lotes y sin necesidad de copias intermedias de datos.\n",
    "\n",
    "El conjunto de operaciones que se pueden utilizar en PyArrow se encuentran en el módulo `pyarrow.compute`, a continuación mostramos las más comunes.\n",
    "\n",
    "> Cabe mencionar que vscode da problemas al reconocer algunos de los métodos y funciones al realizar imports\n",
    "\n",
    "#### Filtrado\n",
    "Usamos la función `filter` y añadimos una condición con los operadores de `pyarrow.compute`. `filter` devuelve una tabla directamente, no afecta a la tabla original. Estos filtros se pueden aplicar también sobre datasets para obtener solamente los datos que pasan el filtro como una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "name: string\n",
       "age: int64\n",
       "city: string\n",
       "----\n",
       "name: [[\"Charlie\",\"David\"]]\n",
       "age: [[35,40]]\n",
       "city: [[\"NY\",\"SF\"]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "# Creamos una tabla para aplicar filtros\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"age\": [25, 30, 35, 40],\n",
    "    \"city\": [\"NY\", \"LA\", \"NY\", \"SF\"]\n",
    "}\n",
    "table = pa.Table.from_pydict(data)\n",
    "\n",
    "pc.filter(table, pc.greater(table['age'], 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de realizar operaciones es mediante expresiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "name: string\n",
       "age: int64\n",
       "city: string\n",
       "----\n",
       "name: [[\"Charlie\",\"David\"]]\n",
       "age: [[35,40]]\n",
       "city: [[\"NY\",\"SF\"]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = pc.field(\"age\") > 30\n",
    "table.filter(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtros compuestos\n",
    "\n",
    "Podemos combinar filtros usando los operadores `and_` y `or_`, o expresiones con los operadores `&` (and), `|` (or) y `~` (not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "name: string\n",
       "age: int64\n",
       "city: string\n",
       "----\n",
       "name: [[\"Charlie\"]]\n",
       "age: [[35]]\n",
       "city: [[\"NY\"]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = (pc.field(\"age\") > 30) & \\\n",
    "       (pc.field('city') == 'NY')\n",
    "\n",
    "table.filter(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregaciones\n",
    "\n",
    "Podemos realizar agregaciones sobre arrays. Estos devuelven un tipo asociado a PyArrow, por lo que para convertirlos a tipos de Python debemos llamar al método `as_py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.mean(table['age']).as_py()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones con Arrays\n",
    "También podemos realizar operaciones aritméticas con arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x7ba38157f400>\n",
       "[\n",
       "  [\n",
       "    50,\n",
       "    60,\n",
       "    70,\n",
       "    80\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_age = pc.multiply(table['age'], 2)\n",
    "double_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenación de Tablas\n",
    "Podemos unir dos tablas que sigan el mismo esquema con `concat_tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "actor: string\n",
       "nominations: int64\n",
       "----\n",
       "actor: [[\"Meryl Streep\",\"Katharine Hepburn\"],[\"Jack Nicholson\",\"Bette Davis\"]]\n",
       "nominations: [[21,12],[12,10]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscar_nominations_1 = pa.table([\n",
    "  [\"Meryl Streep\", \"Katharine Hepburn\"],\n",
    "  [21, 12]\n",
    "], names=[\"actor\", \"nominations\"])\n",
    "\n",
    "oscar_nominations_2 = pa.table([\n",
    "  [\"Jack Nicholson\", \"Bette Davis\"],\n",
    "  [12, 10]\n",
    "], names=[\"actor\", \"nominations\"])\n",
    "\n",
    "pa.concat_tables([oscar_nominations_1, oscar_nominations_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Añadir y Modificar Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "actor: string\n",
       "nominations: int64\n",
       "wonyears: list<item: int64>\n",
       "  child 0, item: int64\n",
       "----\n",
       "actor: [[\"Meryl Streep\",\"Katharine Hepburn\"]]\n",
       "nominations: [[21,12]]\n",
       "wonyears: [[[1980,1983,2012],[1934,1968,1969,1982]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Añadir columnas\n",
    "oscar_nominations = oscar_nominations_1.append_column(\n",
    "    \"wonyears\", pa.array([\n",
    "        [1980, 1983, 2012], \n",
    "        [1934, 1968, 1969, 1982]\n",
    "]))\n",
    "oscar_nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "full_name: string\n",
       "nominations: int64\n",
       "wonyears: list<item: int64>\n",
       "  child 0, item: int64\n",
       "----\n",
       "full_name: [[\"Mary Louise Streep\",\"Katharine Houghton Hepburn\"]]\n",
       "nominations: [[21,12]]\n",
       "wonyears: [[[1980,1983,2012],[1934,1968,1969,1982]]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modificar la columna 0\n",
    "oscar_nominations.set_column(0, \"full_name\", \n",
    "                             pa.array([\"Mary Louise Streep\", \"Katharine Houghton Hepburn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupaciones\n",
    "Podemos realizar agrupaciones en tablas usando el método `group_by` y seguirlo con una operación de agregación con el método `aggregate`.\n",
    "\n",
    "En el siguiente ejemplo, agrupamos por la columna `keys` y aplicamos la funciones `sum` y `mean` a la columna `values`. Las operaciones de agregación pueden agregar opciones como el tercer valor de la tupla que indica la operación de agregación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "keys: string\n",
       "values_sum: int64\n",
       "values_mean: double\n",
       "----\n",
       "keys: [[\"a\",\"b\",\"c\",\"d\",\"e\"]]\n",
       "values_sum: [[31,7,15,1,4]]\n",
       "values_mean: [[15.5,3.5,7.5,1,4]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pa.table([\n",
    "     pa.array([\"a\", \"a\", \"b\", \"b\", \"c\", \"d\", \"e\", \"c\"]),\n",
    "     pa.array([11, 20, 3, 4, 5, 1, 4, 10]),\n",
    "    ], names=[\"keys\", \"values\"])\n",
    "\n",
    "table.group_by(\"keys\").aggregate([(\"values\", \"sum\"), (\"values\", \"mean\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordenar Tablas\n",
    "Podemos ordenar tablas con el método `sort_by` y con una sintaxis similar a la de las operaciones de agregación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "keys: string\n",
       "values: int64\n",
       "----\n",
       "keys: [[\"d\",\"b\",\"b\",\"e\",\"c\",\"c\",\"a\",\"a\"]]\n",
       "values: [[1,3,4,4,5,10,11,20]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.sort_by([(\"values\", \"ascending\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los Datos de Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "### RQ1: Distribution of Questioners\n",
    "We plot the histogram of questioners in Figure 3. The graph shows the number of developers that ask a given num- ber of questions, and its y-axis is in log-scale. From the graph, we notice that most developers (33,907 out of 44,087 developers) only ask one question. Only about 23.1% of the developers ask two or more questions. The number of developers that ask questions reduces exponentially as we consider a higher number of posted questions. Only 1.6% of the developers ask more than 5 questions. \n",
    "\n",
    "The result shows that there are few “regular” questioners on StackOverflow. This is possibly because many questions have already been asked before and users could find answers to them by just looking into the various pages on StackOverflow or other question and answer sites via search engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ2: Distribution of Answerers\n",
    "We plot the histogram of answerers in Figure 3. The graph shows the number of developers that answer a given num- ber of questions and its y-axis is also in log-scale. From the graph, we notice that most developers (28,578 out of 44,087 developers) only answer one question. About one thousand developers (2.3%) do not answer any questions. Only about 35.2% of the developers answer two or more questions. The number of developers that answer questions reduces expo- nentially as we consider a higher number of answers. On- ly 7.8% of the developers answer more than 5 questions. The highest number of questions a developer answers in our dataset is 178. There is only one developer that answers this many questions.\n",
    "\n",
    "Compared with the distribution of questioners, the distribution of answers is different. The number of developers that answer a substantial number of questions (> 5) is more than the number of developers that ask a substantial number of questions (> 5)—3,424 (7.8%) versus 701 (1.6%). This may imply that many developers on StackOverflow are interested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ3: Segregation of StackOverflow Com munity\n",
    "To answer this research question, we investigate the proportion of posts that various developers make that are answers to some questions. We show this in Figure 5. We notice that a majority of developers only ask questions but do not answers them (83.2%, 36,672 developers). Thus, we could divide the StackOverflow community into two groups: people that only ask questions, and those that answer one or more questions. The first group is the majority.\n",
    "\n",
    "We also note another peak in Figure 5: These are developers (2,956 of 44,087 developers), with 50-59% of posts being answers. These correspond to ideal developers that contribute answers to the community as much as requesting answers from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ4: RQ4: Reciprocity in StackOverflow\n",
    "To answer the fourth research question, we investigate the help graph. A help graph is a directed graph, where each developer is a node, and the node corresponding to a developer D1 is linked to that of D2 if D1 answers a question posted by D2. We would like to investigate how often two developers D1 and D2 are connected by two edges, one from D1 to D2 and the other from D2 to D1. We find that there are only a few of such developers (23 pairs). We highlight a few in Table 1. The table contains the identifiers of the helpers and helpees that reciprocate. From the result, we hypothesize that developers tend to help anyone no matter if they have helped him or her before and StackOverflow tends to benefit the community as a whole."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
